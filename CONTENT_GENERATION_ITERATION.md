# ğŸ”„ ì½˜í…ì¸  ìƒì„± Iteration ì „ëµ

## ë¬¸ì œ: ì—ì´ì „íŠ¸ë“¤ì´ ì–´ë–»ê²Œ í˜‘ì—…í• ê¹Œ?

### ì˜µì…˜ 1: í•œ ë²ˆì”© ì‹¤í–‰ (One-Shot)
```
TrendScout â†’ ViralHook â†’ ContentCrafter â†’ EngageCritic â†’ ë°œí–‰
```
- ì¥ì : ë¹ ë¦„, ê°„ë‹¨í•¨
- ë‹¨ì : í’ˆì§ˆ ë³´ì¥ ì–´ë ¤ì›€, í”¼ë“œë°± ë°˜ì˜ ë¶ˆê°€

### ì˜µì…˜ 2: í•©ì˜ ê¸°ë°˜ ë°˜ë³µ (Consensus-Based Iteration) â­ ê¶Œì¥
```
Round 1: Writers ì‘ì„± â†’ Critics í‰ê°€ (ì ìˆ˜: 0.58)
Round 2: Writers ìˆ˜ì • â†’ Critics ì¬í‰ê°€ (ì ìˆ˜: 0.72)
Round 3: Writers ìµœì¢… ìˆ˜ì • â†’ Critics ì¬í‰ê°€ (ì ìˆ˜: 0.83) âœ… ë°œí–‰
```
- ì¥ì : ë†’ì€ í’ˆì§ˆ, í”¼ë“œë°± ë°˜ì˜, ì‹¤ì œ í† ë¡ ì²˜ëŸ¼ ì‘ë™
- ë‹¨ì : ëŠë¦¼ (í•˜ì§€ë§Œ í’ˆì§ˆì´ ë” ì¤‘ìš”!)

---

## âœ… ê¶Œì¥: Multi-Turn Refinement with Consensus

**ì»¨ì…‰**: ì—ì´ì „íŠ¸ë“¤ì´ **í’ˆì§ˆ ê¸°ì¤€ì„ ë§Œì¡±í•  ë•Œê¹Œì§€** ì—¬ëŸ¬ ë²ˆ í† ë¡ í•˜ê³  ê°œì„ 

### í”„ë¡œì„¸ìŠ¤

```python
MAX_ITERATIONS = 5  # ë¬´í•œ ë£¨í”„ ë°©ì§€
MIN_QUALITY_SCORE = 0.75  # ë°œí–‰ ì„ê³„ê°’

def generate_content_with_consensus(agents, topic):
    """
    ì—ì´ì „íŠ¸ë“¤ì´ í•©ì˜ì— ë„ë‹¬í•  ë•Œê¹Œì§€ ë°˜ë³µ.
    
    Returns:
        content: ìµœì¢… ì½˜í…ì¸ 
        iterations: ê±¸ë¦° ë¼ìš´ë“œ ìˆ˜
        final_score: ìµœì¢… í’ˆì§ˆ ì ìˆ˜
    """
    
    # Phase 1: ì•„ì´ë””ì–´ ìˆ˜ì§‘ (Parallel)
    trends = agents["TrendScout"].run(f"Analyze trends for: {topic}")
    ideas = agents["Ideator"].run(f"Brainstorm angles for: {topic}")
    
    # Phase 2: ë°˜ë³µì  ê°œì„  (Iterative Refinement)
    content = None
    scores = None
    
    for iteration in range(1, MAX_ITERATIONS + 1):
        print(f"\n=== Round {iteration} ===")
        
        # Writersê°€ ì‘ì„±/ìˆ˜ì •
        if iteration == 1:
            # ì²« ë²ˆì§¸: ì´ˆì•ˆ ì‘ì„±
            hook = agents["ViralHook"].run(
                f"Create viral hook for: {ideas['best_angle']}"
            )
            body = agents["ContentCrafter"].run(
                f"Write content with hook: {hook}, trend: {trends['top_topic']}"
            )
            content = f"{hook}\n\n{body}"
        else:
            # ì´í›„: í”¼ë“œë°± ê¸°ë°˜ ìˆ˜ì •
            content = agents["ContentCrafter"].run(
                f"Improve this content based on feedback:\n"
                f"Content: {content}\n"
                f"Feedback: {scores['feedback']}"
            )
        
        # Criticsê°€ í‰ê°€ (Parallel)
        clarity_score = agents["ClarityChecker"].run(f"Evaluate clarity: {content}")
        engage_score = agents["EngageCritic"].run(f"Evaluate engagement: {content}")
        safety_check = agents["HRValidation"].run(f"Check safety: {content}")
        
        # ì¢…í•© ì ìˆ˜ ê³„ì‚°
        scores = {
            "clarity": clarity_score["score"],
            "novelty": engage_score["novelty"],
            "shareability": engage_score["shareability"],
            "credibility": engage_score["credibility"],
            "safety": safety_check["score"],
            "overall": (
                clarity_score["score"] * 0.2 +
                engage_score["novelty"] * 0.2 +
                engage_score["shareability"] * 0.3 +
                engage_score["credibility"] * 0.2 +
                safety_check["score"] * 0.1
            ),
            "feedback": [
                clarity_score.get("feedback", ""),
                engage_score.get("feedback", ""),
            ]
        }
        
        print(f"Overall Score: {scores['overall']:.2f}")
        
        # í•©ì˜ ì²´í¬: ê¸°ì¤€ í†µê³¼?
        if scores["overall"] >= MIN_QUALITY_SCORE:
            print(f"âœ… Consensus reached! Publishing after {iteration} rounds.")
            return content, iteration, scores
        
        # ì•ˆì „ì„± ì‹¤íŒ¨í•˜ë©´ ì¦‰ì‹œ ì¤‘ë‹¨
        if scores["safety"] < 0.9:
            print(f"âŒ Safety violation! Aborting.")
            return None, iteration, scores
        
        print(f"âš ï¸ Quality below threshold. Refining...")
    
    # ìµœëŒ€ ë°˜ë³µ ë„ë‹¬
    print(f"â° Max iterations reached. Using best attempt.")
    return content, MAX_ITERATIONS, scores
```

---

## ğŸ¯ ì‹¤ì „ ì˜ˆì‹œ

### Iteration 1 (ì´ˆì•ˆ)

```
[TrendScout] "AI agents are trending, WeaveHack2 getting attention"
[Ideator] "Angle: behind-the-scenes of building agents"
[ViralHook] "Everyone thinks building AI agents is hard. It's not."
[ContentCrafter] "Everyone thinks building AI agents is hard. It's not.
                 Here's what I learned building a multi-agent system
                 for WeaveHack2 ğŸ‘‡"
                 
[ClarityChecker] Score: 0.75 "Clear but generic"
[EngageCritic] Score: 0.68 (novelty: 0.60, shareability: 0.70)
                "Feedback: Hook is good but body is weak. Add specifics."
[HRValidation] Score: 1.0 "Safe"

Overall: 0.71 âŒ Below 0.75 threshold
```

### Iteration 2 (ê°œì„ )

```
[ContentCrafter] (í”¼ë“œë°± ë°˜ì˜)
"Everyone thinks building AI agents is hard. It's not.

I built a self-optimizing agent team for WeaveHack2.
The HR agent fires/hires other agents based on performance.

3 viral tweets later, here's what worked:
- Trend analyzer that scans Twitter every 2 hours
- Contrarian writer that challenges AI hype
- Critic that rejects 70% of drafts

The twist? The agents improved THEMSELVES. 
No human intervention for 48 hours. ğŸ¤¯"

[ClarityChecker] Score: 0.85 "Much clearer, specific examples"
[EngageCritic] Score: 0.82 (novelty: 0.80, shareability: 0.85)
                "Feedback: Strong! Maybe add a surprising stat?"
[HRValidation] Score: 1.0 "Safe"

Overall: 0.83 âœ… Passes threshold!
```

---

## ğŸ”§ êµ¬í˜„: content_generator.py

```python
from typing import Dict, Tuple, Optional
import json

class ContentGenerator:
    def __init__(self, agents: Dict, config: Dict = None):
        self.agents = agents
        self.config = config or {
            "max_iterations": 5,
            "min_quality_score": 0.75,
            "min_safety_score": 0.9
        }
    
    def generate(self, topic: str) -> Tuple[Optional[str], int, Dict]:
        """
        í•©ì˜ ê¸°ë°˜ ì½˜í…ì¸  ìƒì„±.
        
        Returns:
            (content, iterations, scores)
        """
        # Phase 1: Research (Parallel)
        context = self._gather_context(topic)
        
        # Phase 2: Iterative Refinement
        content = None
        scores = None
        
        for i in range(1, self.config["max_iterations"] + 1):
            # Write
            content = self._write_content(content, context, scores, iteration=i)
            
            # Evaluate
            scores = self._evaluate_content(content)
            
            # Check consensus
            if self._is_consensus_reached(scores):
                return content, i, scores
            
            if scores["safety"] < self.config["min_safety_score"]:
                return None, i, scores
        
        return content, self.config["max_iterations"], scores
    
    def _gather_context(self, topic: str) -> Dict:
        """Phase 1: ë³‘ë ¬ë¡œ ì»¨í…ìŠ¤íŠ¸ ìˆ˜ì§‘"""
        context = {}
        
        if "TrendScout" in self.agents:
            context["trends"] = self.agents["TrendScout"].run(
                f"Analyze Twitter trends for: {topic}"
            )
        
        if "Ideator" in self.agents:
            context["ideas"] = self.agents["Ideator"].run(
                f"Brainstorm angles for: {topic}"
            )
        
        return context
    
    def _write_content(
        self, 
        previous_content: Optional[str],
        context: Dict,
        scores: Optional[Dict],
        iteration: int
    ) -> str:
        """Phase 2: ì‘ì„± ë˜ëŠ” ìˆ˜ì •"""
        
        if iteration == 1:
            # ì²« ë²ˆì§¸: ì´ˆì•ˆ
            prompt = f"""Create a viral tweet about: {context.get('topic', 'WeaveHack2')}
            
Trends: {context.get('trends', {})}
Ideas: {context.get('ideas', {})}

Make it attention-grabbing and specific."""
        else:
            # ì´í›„: í”¼ë“œë°± ë°˜ì˜
            prompt = f"""Improve this content based on critic feedback:

Content:
{previous_content}

Feedback:
{json.dumps(scores.get('feedback', {}), indent=2)}

Overall score: {scores.get('overall', 0):.2f} (target: {self.config['min_quality_score']})

Make specific improvements to address the feedback."""
        
        # Writers ì‹¤í–‰ (ìˆœì°¨ ë˜ëŠ” ë³‘ë ¬)
        if "ContentCrafter" in self.agents:
            return self.agents["ContentCrafter"].run(prompt)
        
        return previous_content or "Fallback content"
    
    def _evaluate_content(self, content: str) -> Dict:
        """Phase 2: Criticsê°€ ë³‘ë ¬ í‰ê°€"""
        scores = {
            "clarity": 0.5,
            "novelty": 0.5,
            "shareability": 0.5,
            "credibility": 0.5,
            "safety": 1.0,
            "feedback": []
        }
        
        # ê° critic ë³‘ë ¬ ì‹¤í–‰
        if "ClarityChecker" in self.agents:
            clarity_result = self.agents["ClarityChecker"].run(
                f"Evaluate clarity of: {content}"
            )
            scores["clarity"] = clarity_result.get("score", 0.5)
            scores["feedback"].append(clarity_result.get("feedback", ""))
        
        if "EngageCritic" in self.agents:
            engage_result = self.agents["EngageCritic"].run(
                f"Evaluate engagement of: {content}"
            )
            scores["novelty"] = engage_result.get("novelty", 0.5)
            scores["shareability"] = engage_result.get("shareability", 0.5)
            scores["credibility"] = engage_result.get("credibility", 0.5)
            scores["feedback"].append(engage_result.get("feedback", ""))
        
        if "HRValidation" in self.agents:
            safety_result = self.agents["HRValidation"].run(
                f"Check safety of: {content}"
            )
            scores["safety"] = safety_result.get("score", 1.0)
        
        # Overall ê³„ì‚°
        scores["overall"] = (
            scores["clarity"] * 0.2 +
            scores["novelty"] * 0.2 +
            scores["shareability"] * 0.3 +
            scores["credibility"] * 0.2 +
            scores["safety"] * 0.1
        )
        
        return scores
    
    def _is_consensus_reached(self, scores: Dict) -> bool:
        """í•©ì˜ ë„ë‹¬ ì²´í¬"""
        return (
            scores["overall"] >= self.config["min_quality_score"] and
            scores["safety"] >= self.config["min_safety_score"]
        )


# ì‚¬ìš© ì˜ˆì‹œ
if __name__ == "__main__":
    from agent_factory import apply_hr_decisions, load_team_state
    from hr_validation_agent.agent import analyze_team_and_decide
    import json
    
    # 1. ì—ì´ì „íŠ¸ íŒ€ ìƒì„±
    team_state = load_team_state("examples/mason_weavehack2_empty.json")
    result = analyze_team_and_decide(json.dumps(team_state))
    hr_decisions = json.loads(result)
    
    agents = {}
    agents = apply_hr_decisions(agents, hr_decisions)
    
    # 2. ì½˜í…ì¸  ìƒì„± (í•©ì˜ ê¸°ë°˜)
    generator = ContentGenerator(agents, {
        "max_iterations": 5,
        "min_quality_score": 0.75,
        "min_safety_score": 0.9
    })
    
    content, iterations, scores = generator.generate(
        topic="Building self-optimizing agents for WeaveHack2"
    )
    
    print(f"\nâœ… Content ready after {iterations} rounds!")
    print(f"ğŸ“Š Final score: {scores['overall']:.2f}")
    print(f"\nğŸ“ Content:\n{content}")
```

---

## ğŸ“Š ì„±ëŠ¥ vs í’ˆì§ˆ Trade-off

| ì „ëµ | ì†ë„ | í’ˆì§ˆ | í”¼ë“œë°± ë°˜ì˜ | ê¶Œì¥ ì‚¬ìš© |
|------|------|------|-------------|----------|
| **One-Shot** | âš¡âš¡âš¡ ë¹ ë¦„ | â­â­ ë³´í†µ | âŒ ì—†ìŒ | ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ |
| **Fixed Rounds** (3íšŒ) | âš¡âš¡ ì ë‹¹ | â­â­â­ ì¢‹ìŒ | âœ… ì œí•œì  | ì¼ë°˜ì  ìƒí™© |
| **Consensus-Based** | âš¡ ëŠë¦¼ | â­â­â­â­ ë§¤ìš° ì¢‹ìŒ | âœ…âœ… ì™„ì „ | ë°”ì´ëŸ´ ëª©í‘œ â­ |

**Masonì˜ ê²½ìš°**: ë°”ì´ëŸ´ì´ ëª©í‘œì´ë¯€ë¡œ **Consensus-Based** ê¶Œì¥!

---

## ğŸ¯ í•µì‹¬ í¬ì¸íŠ¸

1. **í’ˆì§ˆ > ì†ë„**: ë°”ì´ëŸ´ ì½˜í…ì¸ ëŠ” 1ê°œë§Œ ìˆì–´ë„ ì¶©ë¶„
2. **í”¼ë“œë°± ë£¨í”„**: Criticsì˜ í”¼ë“œë°±ì„ ë°˜ì˜í•´ì•¼ ê°œì„ ë¨
3. **ì•ˆì „ì¥ì¹˜**: MAX_ITERATIONSë¡œ ë¬´í•œ ë£¨í”„ ë°©ì§€
4. **ëª…í™•í•œ ê¸°ì¤€**: MIN_QUALITY_SCOREë¡œ í•©ì˜ ì •ì˜
5. **ë³‘ë ¬ ì‹¤í–‰**: Context ìˆ˜ì§‘ê³¼ í‰ê°€ëŠ” ë³‘ë ¬ë¡œ

---

**ë²„ì „**: 1.0.0  
**ì—…ë°ì´íŠ¸**: 2025-10-11

